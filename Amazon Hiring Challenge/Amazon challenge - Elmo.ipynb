{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# from tqdm import tqdm \n",
    "# tqdm.pandas(desc=\"progress-bar\")\n",
    "\n",
    "import gensim \n",
    "import string\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "LabeledSentence = gensim.models.doc2vec.LabeledSentence\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "#os.chdir('C:\\\\Users\\\\BTHANISH\\\\Documents\\\\Thanish\\\\Competition\\\\Hacker earth\\\\Amazon Hiring Challenge')\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0810 06:05:19.526812 140651333011264 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5959, 3) (2553, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review_text</th>\n",
       "      <th>Review_Title</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Did nothing for me, didn't help lost even with...</td>\n",
       "      <td>Useless</td>\n",
       "      <td>Shipment and delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Did nothing for me, didn't help lost even with...</td>\n",
       "      <td>Useless</td>\n",
       "      <td>Not Effective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I have bought these bags and  immediately open...</td>\n",
       "      <td>TRASH!!! Do not buy these bags it’s a waist of...</td>\n",
       "      <td>Customer Service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gave me an allergic reaction on my face :(</td>\n",
       "      <td>Do not recommend</td>\n",
       "      <td>Allergic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>These don't compare to the name brand wipes. F...</td>\n",
       "      <td>Can't tackle big messes</td>\n",
       "      <td>Texture</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Review_text  \\\n",
       "0  Did nothing for me, didn't help lost even with...   \n",
       "1  Did nothing for me, didn't help lost even with...   \n",
       "2  I have bought these bags and  immediately open...   \n",
       "3         Gave me an allergic reaction on my face :(   \n",
       "4  These don't compare to the name brand wipes. F...   \n",
       "\n",
       "                                        Review_Title                  topic  \n",
       "0                                            Useless  Shipment and delivery  \n",
       "1                                            Useless          Not Effective  \n",
       "2  TRASH!!! Do not buy these bags it’s a waist of...       Customer Service  \n",
       "3                                   Do not recommend               Allergic  \n",
       "4                            Can't tackle big messes                Texture  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF_train_prod = pd.read_csv('train.csv')\n",
    "DF_test_prod = pd.read_csv('test.csv')\n",
    "#DF_test_prod = DF_test_prod.loc[~DF_test_prod.duplicated(),]\n",
    "\n",
    "print(DF_train_prod.shape, DF_test_prod.shape)\n",
    "\n",
    "DF_test_prod['topic'] = 'Test_dataset'\n",
    "DF_prod = pd.concat([DF_train_prod, DF_test_prod]).reset_index(drop = True)\n",
    "DF_prod.columns = ['Review_text', 'Review_Title', 'topic']\n",
    "DF_prod.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nltk.set_proxy('https://19.12.1.40:83')\n",
    "# nltk.download(['punkt', 'stopwords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review_text</th>\n",
       "      <th>Review_Title</th>\n",
       "      <th>topic</th>\n",
       "      <th>Review_text_tokens</th>\n",
       "      <th>Review_Title_tokens</th>\n",
       "      <th>combined_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nothing didnt help lost even working eating he...</td>\n",
       "      <td>useless</td>\n",
       "      <td>Shipment and delivery</td>\n",
       "      <td>[nothing, didnt, help, lost, even, working, ea...</td>\n",
       "      <td>[useless]</td>\n",
       "      <td>[nothing, didnt, help, lost, even, working, ea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nothing didnt help lost even working eating he...</td>\n",
       "      <td>useless</td>\n",
       "      <td>Not Effective</td>\n",
       "      <td>[nothing, didnt, help, lost, even, working, ea...</td>\n",
       "      <td>[useless]</td>\n",
       "      <td>[nothing, didnt, help, lost, even, working, ea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bought bags immediately open one put trash bag...</td>\n",
       "      <td>trash buy bags waist time</td>\n",
       "      <td>Customer Service</td>\n",
       "      <td>[bought, bags, immediately, open, one, put, tr...</td>\n",
       "      <td>[trash, buy, bags, waist, time]</td>\n",
       "      <td>[bought, bags, immediately, open, one, put, tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gave allergic reaction face</td>\n",
       "      <td>recommend</td>\n",
       "      <td>Allergic</td>\n",
       "      <td>[gave, allergic, reaction, face]</td>\n",
       "      <td>[recommend]</td>\n",
       "      <td>[gave, allergic, reaction, face, recommend]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dont compare name brand wipes family  little k...</td>\n",
       "      <td>cant tackle big messes</td>\n",
       "      <td>Texture</td>\n",
       "      <td>[dont, compare, name, brand, wipes, family, li...</td>\n",
       "      <td>[cant, tackle, big, messes]</td>\n",
       "      <td>[dont, compare, name, brand, wipes, family, li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dont compare name brand wipes family  little k...</td>\n",
       "      <td>cant tackle big messes</td>\n",
       "      <td>Quality/Contaminated</td>\n",
       "      <td>[dont, compare, name, brand, wipes, family, li...</td>\n",
       "      <td>[cant, tackle, big, messes]</td>\n",
       "      <td>[dont, compare, name, brand, wipes, family, li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dont compare name brand wipes family  little k...</td>\n",
       "      <td>cant tackle big messes</td>\n",
       "      <td>Color and texture</td>\n",
       "      <td>[dont, compare, name, brand, wipes, family, li...</td>\n",
       "      <td>[cant, tackle, big, messes]</td>\n",
       "      <td>[dont, compare, name, brand, wipes, family, li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>good</td>\n",
       "      <td>tastes horrible</td>\n",
       "      <td>Bad Taste/Flavor</td>\n",
       "      <td>[good]</td>\n",
       "      <td>[tastes, horrible]</td>\n",
       "      <td>[good, tastes, horrible]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>extremely hard swallow pills huge sides sharp ...</td>\n",
       "      <td>choking hazard</td>\n",
       "      <td>Too big to swallow</td>\n",
       "      <td>[extremely, hard, swallow, pills, huge, sides,...</td>\n",
       "      <td>[choking, hazard]</td>\n",
       "      <td>[extremely, hard, swallow, pills, huge, sides,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>first style leaving review solimo incontinent ...</td>\n",
       "      <td>bring back old style</td>\n",
       "      <td>Quality/Contaminated</td>\n",
       "      <td>[first, style, leaving, review, solimo, incont...</td>\n",
       "      <td>[bring, back, old, style]</td>\n",
       "      <td>[first, style, leaving, review, solimo, incont...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Review_text  \\\n",
       "0  nothing didnt help lost even working eating he...   \n",
       "1  nothing didnt help lost even working eating he...   \n",
       "2  bought bags immediately open one put trash bag...   \n",
       "3                        gave allergic reaction face   \n",
       "4  dont compare name brand wipes family  little k...   \n",
       "5  dont compare name brand wipes family  little k...   \n",
       "6  dont compare name brand wipes family  little k...   \n",
       "7                                               good   \n",
       "8  extremely hard swallow pills huge sides sharp ...   \n",
       "9  first style leaving review solimo incontinent ...   \n",
       "\n",
       "                Review_Title                  topic  \\\n",
       "0                    useless  Shipment and delivery   \n",
       "1                    useless          Not Effective   \n",
       "2  trash buy bags waist time       Customer Service   \n",
       "3                  recommend               Allergic   \n",
       "4     cant tackle big messes                Texture   \n",
       "5     cant tackle big messes   Quality/Contaminated   \n",
       "6     cant tackle big messes      Color and texture   \n",
       "7            tastes horrible       Bad Taste/Flavor   \n",
       "8             choking hazard     Too big to swallow   \n",
       "9       bring back old style   Quality/Contaminated   \n",
       "\n",
       "                                  Review_text_tokens  \\\n",
       "0  [nothing, didnt, help, lost, even, working, ea...   \n",
       "1  [nothing, didnt, help, lost, even, working, ea...   \n",
       "2  [bought, bags, immediately, open, one, put, tr...   \n",
       "3                   [gave, allergic, reaction, face]   \n",
       "4  [dont, compare, name, brand, wipes, family, li...   \n",
       "5  [dont, compare, name, brand, wipes, family, li...   \n",
       "6  [dont, compare, name, brand, wipes, family, li...   \n",
       "7                                             [good]   \n",
       "8  [extremely, hard, swallow, pills, huge, sides,...   \n",
       "9  [first, style, leaving, review, solimo, incont...   \n",
       "\n",
       "               Review_Title_tokens  \\\n",
       "0                        [useless]   \n",
       "1                        [useless]   \n",
       "2  [trash, buy, bags, waist, time]   \n",
       "3                      [recommend]   \n",
       "4      [cant, tackle, big, messes]   \n",
       "5      [cant, tackle, big, messes]   \n",
       "6      [cant, tackle, big, messes]   \n",
       "7               [tastes, horrible]   \n",
       "8                [choking, hazard]   \n",
       "9        [bring, back, old, style]   \n",
       "\n",
       "                                     combined_tokens  \n",
       "0  [nothing, didnt, help, lost, even, working, ea...  \n",
       "1  [nothing, didnt, help, lost, even, working, ea...  \n",
       "2  [bought, bags, immediately, open, one, put, tr...  \n",
       "3        [gave, allergic, reaction, face, recommend]  \n",
       "4  [dont, compare, name, brand, wipes, family, li...  \n",
       "5  [dont, compare, name, brand, wipes, family, li...  \n",
       "6  [dont, compare, name, brand, wipes, family, li...  \n",
       "7                           [good, tastes, horrible]  \n",
       "8  [extremely, hard, swallow, pills, huge, sides,...  \n",
       "9  [first, style, leaving, review, solimo, incont...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = stopwords.words('english')\n",
    "\n",
    "# Tokenizing the two columns Review_text, Review_Title\n",
    "for col in ['Review_text', 'Review_Title']:\n",
    "    #Converting the columns to lowercase\n",
    "    DF_prod[col] = DF_prod[col].apply(lambda x :  x.lower())\n",
    "\n",
    "    #Replacing the puncutations\n",
    "    DF_prod[col] = DF_prod[col].str.replace('[^\\w\\s]','')\n",
    "    \n",
    "    # Removing stop words\n",
    "    DF_prod[col] = DF_prod[col].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "    \n",
    "    # Remove numerics from the data\n",
    "    DF_prod[col] = DF_prod[col].str.replace('\\d+', '')\n",
    "    \n",
    "    #Getting the tokens for the two columns\n",
    "    new_col = col + '_tokens'\n",
    "    DF_prod[new_col] = DF_prod.apply(lambda row : word_tokenize(row[col]), axis = 1)\n",
    "\n",
    "# Combined tokens\n",
    "DF_prod['combined_tokens'] = DF_prod['Review_text_tokens'] + DF_prod['Review_Title_tokens']\n",
    "DF_prod.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the data back to train prod and test prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5959, 6) (2553, 6)\n"
     ]
    }
   ],
   "source": [
    "train_prod = DF_prod.loc[DF_prod.topic != 'Test_dataset',].reset_index(drop = True)\n",
    "test_prod = DF_prod.loc[DF_prod.topic == 'Test_dataset',].reset_index(drop = True)\n",
    "\n",
    "print(train_prod.shape, test_prod.shape)\n",
    "\n",
    "# Get the indep and dep features\n",
    "indep = ['combined_tokens']\n",
    "dep = ['topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert the labels to integer\n",
    "LE = LabelEncoder()\n",
    "LE.fit(train_prod.topic)\n",
    "train_prod.topic = LE.transform(train_prod.topic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Download the elmo model for the first time using proxy\n",
    "# module_url = \"https://tfhub.dev/google/elmo/2\"\n",
    "# print(\"Loading model from {}\".format(module_url))\n",
    "# os.environ[\"https_proxy\"] = \"http://19.12.1.40:83\"\n",
    "# elmo = hub.Module(module_url, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Call the model from the next time\n",
    "elmo = hub.Module('https://tfhub.dev/google/elmo/2', trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on sentenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training on sentenes\n",
    "def elmo_vectors(x):\n",
    "    embeddings = elmo(x.tolist(), signature=\"default\", as_dict=True)[\"elmo\"]\n",
    "    \n",
    "    #return embeddings\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.tables_initializer())\n",
    "        # return average of ELMo features\n",
    "        return sess.run(tf.reduce_mean(embeddings,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train on sentence\n",
    "\n",
    "# threshold = 100\n",
    "# list_train = [train_prod[i:i+threshold] for i in range(0, train_prod.shape[0], threshold)]\n",
    "# list_test  = [test_prod[i:i+threshold] for i in range(0, test_prod.shape[0], threshold)]\n",
    "\n",
    "# # Extract ELMo embeddings\n",
    "# elmo_train = [elmo_vectors(x['Review_text']) for x in list_train]\n",
    "# elmo_test = [elmo_vectors(x['Review_text']) for x in list_test]\n",
    "\n",
    "# elmo_train_prod = np.concatenate([elmo_train], axis = 0)\n",
    "# elmo_test_prod = np.concatenate([elmo_test], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # save elmo_train_new\n",
    "# pickle_out = open(\"elmo_train_10082019.pickle\",\"wb\")\n",
    "# pickle.dump(elmo_train_prod, pickle_out)\n",
    "# pickle_out.close()\n",
    "\n",
    "# # save elmo_test_new\n",
    "# pickle_out = open(\"elmo_test_10082019.pickle\",\"wb\")\n",
    "# pickle.dump(elmo_test_prod, pickle_out)\n",
    "# pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def elmo_vectors(x):\n",
    "    token_input = x\n",
    "    tokens_length = [x.shape[0], max_length_sequence]    \n",
    "    embeddings = elmo(inputs = {\"tokens\": tokens_input ,\"sequence_len\": tokens_length}, \n",
    "                      signature=\"tokens\", as_dict=True)[\"elmo\"]\n",
    "    \n",
    "    #return embeddings\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.tables_initializer())\n",
    "        # return average of ELMo features\n",
    "        return sess.run(tf.reduce_mean(embeddings,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train on tokens\n",
    "threshold = 100\n",
    "list_train = [train_prod[i:i+threshold] for i in range(0, train_prod.shape[0], threshold)]\n",
    "list_test  = [test_prod[i:i+threshold] for i in range(0, test_prod.shape[0], threshold)]\n",
    "\n",
    "# Extract ELMo embeddings\n",
    "max_length_sequence = train_prod['combined_tokens'].apply(len).max() #max length of the sentence in the corpus i.e.(maximum word in a sentence)\n",
    "\n",
    "elmo_train = [elmo_vectors(x['combined_tokens']) for x in list_train]\n",
    "elmo_test = [elmo_vectors(x['combined_tokens']) for x in list_test]\n",
    "\n",
    "elmo_train_prod = np.concatenate([elmo_train], axis = 0)\n",
    "elmo_test_prod = np.concatenate([elmo_test], axis = 0)\n",
    "\n",
    "print(\"Train shape\",elmo_train_prod.shape, \"Test shape\", elmo_test_prod.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # save elmo_train_new\n",
    "# pickle_out = open(\"elmo_train_tokens_10082019.pickle\",\"wb\")\n",
    "# pickle.dump(elmo_train_prod, pickle_out)\n",
    "# pickle_out.close()\n",
    "\n",
    "# # save elmo_test_new\n",
    "# pickle_out = open(\"elmo_test_tokens_10082019.pickle\",\"wb\")\n",
    "# pickle.dump(elmo_test_prod, pickle_out)\n",
    "# pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading elmo models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60,), (26,))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle_in = open(\"elmo_train_10082019.pickle\", \"rb\")\n",
    "#pickle_in = open(\"elmo_train_tokens_10082019.pickle\", \"rb\") # if you need tokens\n",
    "elmo_train_prod_DF = pickle.load(pickle_in)\n",
    "\n",
    "# load elmo_train_new\n",
    "pickle_in = open(\"elmo_test_10082019.pickle\", \"rb\")\n",
    "#pickle_in = open(\"elmo_test_tokens_10082019.pickle\", \"rb\") # if you need tokens\n",
    "elmo_test_prod_DF = pickle.load(pickle_in)\n",
    "\n",
    "elmo_train_prod_DF.shape, elmo_test_prod_DF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00412273, -0.05918864,  0.06556313, ...,  0.01515634,\n",
       "         0.01788805, -0.0090268 ],\n",
       "       [-0.00412273, -0.05918864,  0.06556313, ...,  0.01515634,\n",
       "         0.01788805, -0.0090268 ],\n",
       "       [ 0.01752855,  0.01659176,  0.07129052, ...,  0.03331851,\n",
       "        -0.07741871,  0.04769161],\n",
       "       ...,\n",
       "       [-0.00112581, -0.03386815,  0.03087963, ...,  0.0358163 ,\n",
       "        -0.01794741,  0.01692263],\n",
       "       [-0.00112581, -0.03386815,  0.03087963, ...,  0.0358163 ,\n",
       "        -0.01794741,  0.01692263],\n",
       "       [-0.00112581, -0.03386815,  0.03087963, ...,  0.0358163 ,\n",
       "        -0.01794741,  0.01692263]], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elmo_train_prod_DF[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5959, 1024), (2553, 1024))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reshaping the arrays \n",
    "\n",
    "def reshaping_DF (DF):\n",
    "    a = np.empty([0,1024], )\n",
    "    for i in range(DF.shape[0]):\n",
    "    #     print(elmo_train_prod[lisi].shape)\n",
    "    #     print('Head')\n",
    "    #     print(elmo_train_prod[lisi][:3])\n",
    "    #     print('tail')\n",
    "    #     print(elmo_train_prod[lisi][-3:])\n",
    "\n",
    "    #     print(\"\")\n",
    "        a = np.concatenate((a, DF[i]), axis=0)\n",
    "    #print(a.shape)\n",
    "    return a\n",
    "\n",
    "\n",
    "elmo_train_prod = reshaping_DF(elmo_train_prod_DF)\n",
    "elmo_test_prod = reshaping_DF(elmo_test_prod_DF)\n",
    "elmo_train_prod.shape, elmo_test_prod.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4171, 750) (4171, 1) (1788, 750) (1788, 1) (5959, 1024) (2553, 1024)\n"
     ]
    }
   ],
   "source": [
    "train_stop = int(train_prod.shape[0]*0.7)\n",
    "\n",
    "np.random.seed(100)\n",
    "# train_local_X, test_local_X, train_local_Y, test_local_Y = train_test_split(train_prod[indep],\n",
    "#                                                                             train_prod[dep],\n",
    "#                                                                             test_size = 0.2)\n",
    "train_local_X = elmo_train_prod[0 : train_stop, 0:750]\n",
    "train_local_Y = train_prod[dep][0 : train_stop]\n",
    "test_local_X = elmo_train_prod[train_stop : train_prod.shape[0], 0:750]\n",
    "test_local_Y = train_prod[dep][train_stop : train_prod.shape[0]]\n",
    "\n",
    "train_prod_X = elmo_train_prod\n",
    "train_prod_Y = train_prod[dep]\n",
    "test_prod_X = elmo_test_prod\n",
    "\n",
    "print(train_local_X.shape, train_local_Y.shape, test_local_X.shape, test_local_Y.shape, train_prod_X.shape, test_prod_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForest "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training on Local "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39988814317673377\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False    1073\n",
       "True      715\n",
       "Name: topic, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(100)\n",
    "RF = RandomForestClassifier(n_estimators = 100)\n",
    "RF.fit(train_local_X, train_local_Y)\n",
    "RF_local_pred = RF.predict(test_local_X)\n",
    "\n",
    "print(sum(RF_local_pred == test_local_Y.topic)/len(test_local_Y.topic))\n",
    "(RF_local_pred == test_local_Y.topic).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40548098434004476\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False    1063\n",
       "True      725\n",
       "Name: topic, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(100)\n",
    "RF = RandomForestClassifier(n_estimators = 110)\n",
    "RF.fit(train_local_X, train_local_Y)\n",
    "RF_local_pred = RF.predict(test_local_X)\n",
    "print(sum(RF_local_pred == test_local_Y.topic)/len(test_local_Y.topic))\n",
    "(RF_local_pred == test_local_Y.topic).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### on prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # test prod\n",
    "# RF_sub = DF_test_prod.copy()\n",
    "# RF_prod_pred = RF.predict(test_prod_vecs_w2v)\n",
    "# RF_sub['topic'] = LE.inverse_transform(RF_prod_pred)\n",
    "# RF_sub.to_csv('RF_sub_4.csv', index = False)\n",
    "# #RF_sub.tail(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RF Test local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    920\n",
      "0    868\n",
      "Name: predicted_int, dtype: int64\n",
      "Accuracy : 0.5145413870246085\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>Actual</th>\n",
       "      <th>predicted</th>\n",
       "      <th>seq</th>\n",
       "      <th>predicted_trim</th>\n",
       "      <th>predicted_int</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['ive', 'received', 'minutes', 'ago', 'opened'...</td>\n",
       "      <td>Shipment and delivery</td>\n",
       "      <td>[Packaging, Shipment and delivery, Customer Se...</td>\n",
       "      <td>2</td>\n",
       "      <td>[Packaging, Shipment and delivery]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['ive', 'received', 'minutes', 'ago', 'opened'...</td>\n",
       "      <td>Packaging</td>\n",
       "      <td>[Packaging, Shipment and delivery, Customer Se...</td>\n",
       "      <td>2</td>\n",
       "      <td>[Packaging, Shipment and delivery]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['disappointed', 'wasted', 'terrible', 'produc...</td>\n",
       "      <td>Smells Bad</td>\n",
       "      <td>[Bad Taste/Flavor, Quality/Contaminated, Not E...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Bad Taste/Flavor]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['broken', 'fibers', 'ratio', 'much', 'higher'...</td>\n",
       "      <td>Packaging</td>\n",
       "      <td>[Not Effective, Allergic, Packaging, Bad Taste...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Not Effective]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['pills', 'much', 'large', 'better', 'take', '...</td>\n",
       "      <td>Too big to swallow</td>\n",
       "      <td>[Too big to swallow, Not Effective, Quality/Co...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Too big to swallow]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens                 Actual  \\\n",
       "0  ['ive', 'received', 'minutes', 'ago', 'opened'...  Shipment and delivery   \n",
       "1  ['ive', 'received', 'minutes', 'ago', 'opened'...              Packaging   \n",
       "2  ['disappointed', 'wasted', 'terrible', 'produc...             Smells Bad   \n",
       "3  ['broken', 'fibers', 'ratio', 'much', 'higher'...              Packaging   \n",
       "4  ['pills', 'much', 'large', 'better', 'take', '...     Too big to swallow   \n",
       "\n",
       "                                           predicted  seq  \\\n",
       "0  [Packaging, Shipment and delivery, Customer Se...    2   \n",
       "1  [Packaging, Shipment and delivery, Customer Se...    2   \n",
       "2  [Bad Taste/Flavor, Quality/Contaminated, Not E...    1   \n",
       "3  [Not Effective, Allergic, Packaging, Bad Taste...    1   \n",
       "4  [Too big to swallow, Not Effective, Quality/Co...    1   \n",
       "\n",
       "                       predicted_trim predicted_int  \n",
       "0  [Packaging, Shipment and delivery]             1  \n",
       "1  [Packaging, Shipment and delivery]             1  \n",
       "2                  [Bad Taste/Flavor]             0  \n",
       "3                     [Not Effective]             0  \n",
       "4                [Too big to swallow]             1  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test local\n",
    "RF_local_pred = RF.predict_proba(test_local_X)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_local_X = train_prod.loc[0 : train_stop, ]\n",
    "train_local_Y = train_prod[dep][0 : train_stop]\n",
    "test_local_X = train_prod.loc[train_stop : train_prod.shape[0], ]\n",
    "test_local_Y = train_prod[dep][train_stop : train_prod.shape[0]]\n",
    "\n",
    "train_prod_X = elmo_train_prod\n",
    "train_prod_Y = train_prod[dep]\n",
    "test_prod_X = elmo_test_prod\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_local_DF = pd.DataFrame({'tokens': test_local_X.combined_tokens,\n",
    "                              'Actual' : LE.inverse_transform(test_local_Y.topic)\n",
    "                             })\n",
    "test_local_DF.tokens = test_local_DF.tokens.astype('str')\n",
    "\n",
    "top_n_predictions = np.fliplr(np.argsort(RF_local_pred, axis = 1))\n",
    "#top_class = LE.inverse_transform(RF.classes_[top_n_predictions])\n",
    "top_class = np.array([LE.inverse_transform(i) for i in RF.classes_[top_n_predictions]])\n",
    "\n",
    "test_local_DF['predicted'] = top_class.tolist()\n",
    "\n",
    "temp = test_local_DF.groupby(['tokens'])['Actual'].count().reset_index(drop = False).rename(columns = {'Actual':'seq'})\n",
    "#print(temp.head())\n",
    "\n",
    "#print(test_local.shape)\n",
    "test_local_DF = test_local_DF.merge(temp, how = 'left', \n",
    "                              left_on = ['tokens'], right_on = ['tokens'])\n",
    "#print(test_local.shape)\n",
    "\n",
    "final_list = []\n",
    "test_local_DF['predicted_trim'] = None\n",
    "for index, row in test_local_DF.iterrows():\n",
    "    temp = test_local_DF['predicted'][index][0: (test_local_DF['seq'][index])]\n",
    "    final_list.append(temp)\n",
    "test_local_DF['predicted_trim'] = final_list\n",
    "    \n",
    "\n",
    "final_list = []\n",
    "for index, row in test_local_DF.iterrows():\n",
    "    temp = np.isin(test_local_DF.Actual[index], (test_local_DF['predicted_trim'][index])).astype('int')\n",
    "    final_list.append(temp)\n",
    "test_local_DF['predicted_int'] = final_list\n",
    "    \n",
    "accuracy = sum(test_local_DF.predicted_int) / len(test_local_DF.predicted_int)\n",
    "print(test_local_DF.predicted_int.value_counts())\n",
    "print(\"Accuracy :\",accuracy)\n",
    "\n",
    "test_local_DF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training on prod "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "RF = RandomForestClassifier(n_estimators = 80)\n",
    "RF.fit(train_prod_vecs_w2v, train_prod_Y)\n",
    "RF_local_pred = RF.predict(test_local_vecs_w2v)\n",
    "print(sum(RF_local_pred == test_local_Y.topic)/len(test_local_Y.topic))\n",
    "(RF_local_pred == test_local_Y.topic).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RF Test prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test prod\n",
    "RF_prod_pred = RF.predict_proba(test_local_X)\n",
    "DF_test_prod = pd.read_csv('test.csv')\n",
    "\n",
    "top_n_predictions = np.fliplr(np.argsort(RF_prod_pred, axis = 1))\n",
    "top_class = LE.inverse_transform(RF.classes_[top_n_predictions])\n",
    "\n",
    "DF_test_prod['topic'] = top_class.tolist()\n",
    "DF_test_prod['topic_new'] = top_class.tolist()\n",
    "DF_test_prod = DF_test_prod.loc[~DF_test_prod['Review Text'].duplicated(),]\n",
    "\n",
    "DF_test_prod.head()\n",
    "\n",
    "submission = pd.read_csv('Test.csv')\n",
    "submission['seq'] = submission.groupby(['Review Text']).cumcount()\n",
    "\n",
    "print(submission.shape, DF_test_prod.shape)\n",
    "#submission.head()\n",
    "\n",
    "submission = submission.merge(DF_test_prod[['Review Text', 'topic']], how = 'left',\n",
    "                              left_on = ['Review Text'], right_on = ['Review Text'] )\n",
    "submission.head(10)\n",
    "\n",
    "final_list = []\n",
    "for index, row in submission.iterrows():\n",
    "#     print(index)\n",
    "#     print(submission['seq'][index])\n",
    "#     print(\"\")\n",
    "#     print(submission['topic'][index][submission['seq'][index]])\n",
    "    temp = submission['topic'][index][submission['seq'][index]]\n",
    "#   temp = submission['topic'][index][0]\n",
    "    final_list.append(temp)\n",
    "submission['topic'] = final_list\n",
    "submission.drop(['seq'], axis =1, inplace = True)\n",
    "\n",
    "submission.to_csv('RF_sub_7.csv', index = False)\n",
    "submission.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training on local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "np.random.seed(100)\n",
    "GBM = GradientBoostingClassifier(n_estimators=100)\n",
    "GBM.fit(train_local_vecs_w2v, train_local_Y)\n",
    "GBM_local_pred = GBM.predict(test_local_vecs_w2v)\n",
    "print(sum(GBM_local_pred == test_local_Y.topic)/len(test_local_Y.topic))\n",
    "(GBM_local_pred == test_local_Y.topic).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "np.random.seed(100)\n",
    "GBM = GradientBoostingClassifier(n_estimators=120)\n",
    "GBM.fit(train_local_vecs_w2v, train_local_Y)\n",
    "GBM_local_pred = GBM.predict(test_local_vecs_w2v)\n",
    "print(sum(GBM_local_pred == test_local_Y.topic)/len(test_local_Y.topic))\n",
    "(GBM_local_pred == test_local_Y.topic).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GBM_sub = pd.read_csv('test.csv')\n",
    "# GBM_prod_pred = GBM.predict(test_prod_vecs_w2v)\n",
    "# GBM_sub['topic'] = LE.inverse_transform(GBM_prod_pred)\n",
    "# GBM_sub.to_csv('GBM_sub_6.csv', index = False)\n",
    "# GBM_sub.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GBM Test local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GBM_local_pred = GBM.predict_proba(test_local_vecs_w2v)\n",
    "\n",
    "test_local_DF = pd.DataFrame({'tokens': test_local_X.combined_tokens,\n",
    "                              'Actual' : LE.inverse_transform(test_local_Y.topic)\n",
    "                             })\n",
    "test_local_DF.tokens = test_local_DF.tokens.astype('str')\n",
    "\n",
    "top_n_predictions = np.fliplr(np.argsort(GBM_local_pred, axis = 1))\n",
    "top_class = LE.inverse_transform(GBM.classes_[top_n_predictions])\n",
    "test_local_DF['predicted'] = top_class.tolist()\n",
    "\n",
    "temp = test_local_DF.groupby(['tokens'])['Actual'].count().reset_index(drop = False).rename(columns = {'Actual':'seq'})\n",
    "#print(temp.head())\n",
    "\n",
    "#print(test_local.shape)\n",
    "test_local_DF = test_local_DF.merge(temp, how = 'left', \n",
    "                                    left_on = ['tokens'], right_on = ['tokens'])\n",
    "#print(test_local.shape)\n",
    "\n",
    "final_list = []\n",
    "test_local_DF['predicted_trim'] = None\n",
    "for index, row in test_local_DF.iterrows():\n",
    "    temp = test_local_DF['predicted'][index][0: (test_local_DF['seq'][index])]\n",
    "    final_list.append(temp)\n",
    "test_local_DF['predicted_trim'] = final_list\n",
    "    \n",
    "final_list = []\n",
    "for index, row in test_local_DF.iterrows():\n",
    "    temp = np.isin(test_local_DF.Actual[index], (test_local_DF['predicted_trim'][index])).astype('int')\n",
    "    final_list.append(temp)\n",
    "test_local_DF['predicted_int'] = final_list\n",
    "    \n",
    "accuracy = sum(test_local_DF.predicted_int) / len(test_local_DF.predicted_int)\n",
    "print(test_local_DF.predicted_int.value_counts())\n",
    "print(\"Accuracy :\",accuracy)\n",
    "\n",
    "test_local_DF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training on prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "np.random.seed(100)\n",
    "GBM = GradientBoostingClassifier(n_estimators=100)\n",
    "GBM.fit(train_prod_vecs_w2v, train_prod_Y)\n",
    "GBM_local_pred = GBM.predict(test_local_vecs_w2v)\n",
    "print(sum(GBM_local_pred == test_local_Y.topic)/len(test_local_Y.topic))\n",
    "(GBM_local_pred == test_local_Y.topic).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GBM Test prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GBM_prod_pred = GBM.predict_proba(test_prod_vecs_w2v)\n",
    "DF_test_prod = pd.read_csv('test.csv')\n",
    "\n",
    "top_n_predictions = np.fliplr(np.argsort(GBM_prod_pred, axis = 1))\n",
    "top_class = GBM.classes_[top_n_predictions]\n",
    "DF_test_prod['topic'] = top_class.tolist()\n",
    "DF_test_prod = DF_test_prod.loc[~DF_test_prod['Review Text'].duplicated(),]\n",
    "\n",
    "DF_test_prod.head()\n",
    "\n",
    "submission = pd.read_csv('Test.csv')\n",
    "submission['seq'] = submission.groupby(['Review Text']).cumcount()\n",
    "\n",
    "print(submission.shape, DF_test_prod.shape)\n",
    "#submission.head()\n",
    "\n",
    "submission = submission.merge(DF_test_prod[['Review Text', 'topic']], how = 'left',\n",
    "                              left_on = ['Review Text'], right_on = ['Review Text'] )\n",
    "submission.head(10)\n",
    "\n",
    "final_list = []\n",
    "for index, row in submission.iterrows():\n",
    "#     print(index)\n",
    "#     print(submission['seq'][index])\n",
    "#     print(\"\")\n",
    "#     print(submission['topic'][index][submission['seq'][index]])\n",
    "    temp = submission['topic'][index][submission['seq'][index]]\n",
    "#   temp = submission['topic'][index][0]\n",
    "    final_list.append(temp)\n",
    "submission['topic'] = final_list\n",
    "submission['topic'] = LE.inverse_transform(submission['topic'])\n",
    "submission.drop(['seq'], axis =1, inplace = True)\n",
    "\n",
    "submission.to_csv('GBM_sub_9.csv', index = False)\n",
    "submission.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_local_X.shape, train_local_Y.topic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtrain_local = xgb.DMatrix(data = train_local_X, label = train_local_Y.topic)\n",
    "dtest_local = xgb.DMatrix(data = test_local_X, label = test_local_Y.topic)\n",
    "dtrain_prod = xgb.DMatrix(data = train_prod_X, label = train_prod_Y.topic)\n",
    "dtest_prod = xgb.DMatrix(data = test_prod_X)\n",
    "\n",
    "\n",
    "num_rounds = 1000\n",
    "\n",
    "params = {'objective' : 'multi:softprob',\n",
    "          'num_class' : len(train_local_Y.topic.unique()),\n",
    "          #'eval_metric': 'auc',\n",
    "          'max_depth' : 6,\n",
    "          'eta' : 0.3,\n",
    "          'subsample': 1,\n",
    "          'colsample_bytree': 1,\n",
    "          'silent' : 1\n",
    "          ,'tree_method' : 'gpu_hist'\n",
    "          }\n",
    "\n",
    "eval_set = [(dtrain_local,'train'), (dtest_local,'test')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Stopping. Best iteration:\n",
    "#[106]\ttrain-merror:0.288111\ttest-merror:0.488255\n",
    "        \n",
    "np.random.seed(100)\n",
    "num_rounds = 1000\n",
    "np.random.seed(100)\n",
    "xgb_model = xgb.train(params,\n",
    "                      dtrain_local,\n",
    "                      evals = eval_set,\n",
    "                      num_boost_round = num_rounds,\n",
    "                      #feval = custom_mse,\n",
    "                      verbose_eval = True\n",
    "                      ,early_stopping_rounds = 30\n",
    "                     )\n",
    "\n",
    "xgb_local_pred = xgb_model.predict(dtest_local)\n",
    "\n",
    "#Feature importance\n",
    "pd.DataFrame.from_dict(xgb_model.get_score(), orient = 'index').rename(columns = {0:'importance'}).sort_values(['importance'], ascending = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "xgb_model_prod = xgb.train(params,\n",
    "                           dtrain_prod,\n",
    "                           #evals = eval_set,\n",
    "                           num_boost_round = xgb_model.best_iteration + 10\n",
    "                           #feval = custom_mse,\n",
    "                           verbose_eval = True,\n",
    "                           #early_stopping_rounds = 20\n",
    "                          )\n",
    "\n",
    "\n",
    "xgb_prod_pred = xgb_model_prod.predict(dtest_prod)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Bad Taste/Flavor'], dtype=object)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "top_n_predictions = np.fliplr(np.argsort(xgb_prod_pred, axis = 1))\n",
    "#top_class = LE.inverse_transform(top_n_predictions)\n",
    "# DF_test_prod['topic'] = top_class.tolist()\n",
    "# DF_test_prod = DF_test_prod.loc[~DF_test_prod['Review Text'].duplicated(),]\n",
    "top_n_predictions\n",
    "LE.inverse_transform(top_n_predictions[0:1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGB_prod_pred = xgb_model_prod.predict_proba(test_prod_vecs_w2v)\n",
    "DF_test_prod = pd.read_csv('test.csv')\n",
    "\n",
    "top_n_predictions = np.fliplr(np.argsort(xgb_prod_pred, axis = 1))\n",
    "top_class = LE.inverse_transform(top_n_predictions)\n",
    "DF_test_prod['topic'] = top_class.tolist()\n",
    "DF_test_prod = DF_test_prod.loc[~DF_test_prod['Review Text'].duplicated(),]\n",
    "\n",
    "DF_test_prod.head()\n",
    "\n",
    "submission = pd.read_csv('Test.csv')\n",
    "submission['seq'] = submission.groupby(['Review Text']).cumcount()\n",
    "\n",
    "print(submission.shape, DF_test_prod.shape)\n",
    "#submission.head()\n",
    "\n",
    "submission = submission.merge(DF_test_prod[['Review Text', 'topic']], how = 'left',\n",
    "                              left_on = ['Review Text'], right_on = ['Review Text'] )\n",
    "submission.head(10)\n",
    "\n",
    "final_list = []\n",
    "for index, row in submission.iterrows():\n",
    "#     print(index)\n",
    "#     print(submission['seq'][index])\n",
    "#     print(\"\")\n",
    "#     print(submission['topic'][index][submission['seq'][index]])\n",
    "    temp = submission['topic'][index][submission['seq'][index]]\n",
    "#   temp = submission['topic'][index][0]\n",
    "    final_list.append(temp)\n",
    "submission['topic'] = final_list\n",
    "#submission['topic'] = LE.inverse_transform(submission['topic'])\n",
    "submission.drop(['seq'], axis =1, inplace = True)\n",
    "\n",
    "submission.to_csv('XGB_sub_9.csv', index = False)\n",
    "submission.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
