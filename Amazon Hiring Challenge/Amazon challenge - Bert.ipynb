{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandasql as ps\n",
    "from tqdm import tqdm \n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "\n",
    "import gensim \n",
    "import string\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "LabeledSentence = gensim.models.doc2vec.LabeledSentence\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "os.chdir('C:\\\\Users\\\\BTHANISH\\\\Documents\\\\Thanish\\\\Competition\\\\Hacker earth\\\\Amazon Hiring Challenge')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "#import tensorflow_hub as hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bert_embedding import BertEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5959, 3) (2553, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review_text</th>\n",
       "      <th>Review_Title</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Did nothing for me, didn't help lost even with...</td>\n",
       "      <td>Useless</td>\n",
       "      <td>Shipment and delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Did nothing for me, didn't help lost even with...</td>\n",
       "      <td>Useless</td>\n",
       "      <td>Not Effective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I have bought these bags and  immediately open...</td>\n",
       "      <td>TRASH!!! Do not buy these bags it’s a waist of...</td>\n",
       "      <td>Customer Service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gave me an allergic reaction on my face :(</td>\n",
       "      <td>Do not recommend</td>\n",
       "      <td>Allergic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>These don't compare to the name brand wipes. F...</td>\n",
       "      <td>Can't tackle big messes</td>\n",
       "      <td>Texture</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Review_text  \\\n",
       "0  Did nothing for me, didn't help lost even with...   \n",
       "1  Did nothing for me, didn't help lost even with...   \n",
       "2  I have bought these bags and  immediately open...   \n",
       "3         Gave me an allergic reaction on my face :(   \n",
       "4  These don't compare to the name brand wipes. F...   \n",
       "\n",
       "                                        Review_Title                  topic  \n",
       "0                                            Useless  Shipment and delivery  \n",
       "1                                            Useless          Not Effective  \n",
       "2  TRASH!!! Do not buy these bags it’s a waist of...       Customer Service  \n",
       "3                                   Do not recommend               Allergic  \n",
       "4                            Can't tackle big messes                Texture  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF_train_prod = pd.read_csv('train.csv')\n",
    "DF_test_prod = pd.read_csv('test.csv')\n",
    "#DF_test_prod = DF_test_prod.loc[~DF_test_prod.duplicated(),]\n",
    "\n",
    "print(DF_train_prod.shape, DF_test_prod.shape)\n",
    "\n",
    "DF_test_prod['topic'] = 'Test_dataset'\n",
    "DF_prod = pd.concat([DF_train_prod, DF_test_prod]).reset_index(drop = True)\n",
    "DF_prod.columns = ['Review_text', 'Review_Title', 'topic']\n",
    "DF_prod.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review_text</th>\n",
       "      <th>Review_Title</th>\n",
       "      <th>topic</th>\n",
       "      <th>Review_text_tokens</th>\n",
       "      <th>Review_Title_tokens</th>\n",
       "      <th>combined_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nothing didnt help lost even working eating he...</td>\n",
       "      <td>useless</td>\n",
       "      <td>Shipment and delivery</td>\n",
       "      <td>[nothing, didnt, help, lost, even, working, ea...</td>\n",
       "      <td>[useless]</td>\n",
       "      <td>[nothing, didnt, help, lost, even, working, ea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nothing didnt help lost even working eating he...</td>\n",
       "      <td>useless</td>\n",
       "      <td>Not Effective</td>\n",
       "      <td>[nothing, didnt, help, lost, even, working, ea...</td>\n",
       "      <td>[useless]</td>\n",
       "      <td>[nothing, didnt, help, lost, even, working, ea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bought bags immediately open one put trash bag...</td>\n",
       "      <td>trash buy bags waist time</td>\n",
       "      <td>Customer Service</td>\n",
       "      <td>[bought, bags, immediately, open, one, put, tr...</td>\n",
       "      <td>[trash, buy, bags, waist, time]</td>\n",
       "      <td>[bought, bags, immediately, open, one, put, tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gave allergic reaction face</td>\n",
       "      <td>recommend</td>\n",
       "      <td>Allergic</td>\n",
       "      <td>[gave, allergic, reaction, face]</td>\n",
       "      <td>[recommend]</td>\n",
       "      <td>[gave, allergic, reaction, face, recommend]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dont compare name brand wipes family  little k...</td>\n",
       "      <td>cant tackle big messes</td>\n",
       "      <td>Texture</td>\n",
       "      <td>[dont, compare, name, brand, wipes, family, li...</td>\n",
       "      <td>[cant, tackle, big, messes]</td>\n",
       "      <td>[dont, compare, name, brand, wipes, family, li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dont compare name brand wipes family  little k...</td>\n",
       "      <td>cant tackle big messes</td>\n",
       "      <td>Quality/Contaminated</td>\n",
       "      <td>[dont, compare, name, brand, wipes, family, li...</td>\n",
       "      <td>[cant, tackle, big, messes]</td>\n",
       "      <td>[dont, compare, name, brand, wipes, family, li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dont compare name brand wipes family  little k...</td>\n",
       "      <td>cant tackle big messes</td>\n",
       "      <td>Color and texture</td>\n",
       "      <td>[dont, compare, name, brand, wipes, family, li...</td>\n",
       "      <td>[cant, tackle, big, messes]</td>\n",
       "      <td>[dont, compare, name, brand, wipes, family, li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>good</td>\n",
       "      <td>tastes horrible</td>\n",
       "      <td>Bad Taste/Flavor</td>\n",
       "      <td>[good]</td>\n",
       "      <td>[tastes, horrible]</td>\n",
       "      <td>[good, tastes, horrible]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>extremely hard swallow pills huge sides sharp ...</td>\n",
       "      <td>choking hazard</td>\n",
       "      <td>Too big to swallow</td>\n",
       "      <td>[extremely, hard, swallow, pills, huge, sides,...</td>\n",
       "      <td>[choking, hazard]</td>\n",
       "      <td>[extremely, hard, swallow, pills, huge, sides,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>first style leaving review solimo incontinent ...</td>\n",
       "      <td>bring back old style</td>\n",
       "      <td>Quality/Contaminated</td>\n",
       "      <td>[first, style, leaving, review, solimo, incont...</td>\n",
       "      <td>[bring, back, old, style]</td>\n",
       "      <td>[first, style, leaving, review, solimo, incont...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Review_text  \\\n",
       "0  nothing didnt help lost even working eating he...   \n",
       "1  nothing didnt help lost even working eating he...   \n",
       "2  bought bags immediately open one put trash bag...   \n",
       "3                        gave allergic reaction face   \n",
       "4  dont compare name brand wipes family  little k...   \n",
       "5  dont compare name brand wipes family  little k...   \n",
       "6  dont compare name brand wipes family  little k...   \n",
       "7                                               good   \n",
       "8  extremely hard swallow pills huge sides sharp ...   \n",
       "9  first style leaving review solimo incontinent ...   \n",
       "\n",
       "                Review_Title                  topic  \\\n",
       "0                    useless  Shipment and delivery   \n",
       "1                    useless          Not Effective   \n",
       "2  trash buy bags waist time       Customer Service   \n",
       "3                  recommend               Allergic   \n",
       "4     cant tackle big messes                Texture   \n",
       "5     cant tackle big messes   Quality/Contaminated   \n",
       "6     cant tackle big messes      Color and texture   \n",
       "7            tastes horrible       Bad Taste/Flavor   \n",
       "8             choking hazard     Too big to swallow   \n",
       "9       bring back old style   Quality/Contaminated   \n",
       "\n",
       "                                  Review_text_tokens  \\\n",
       "0  [nothing, didnt, help, lost, even, working, ea...   \n",
       "1  [nothing, didnt, help, lost, even, working, ea...   \n",
       "2  [bought, bags, immediately, open, one, put, tr...   \n",
       "3                   [gave, allergic, reaction, face]   \n",
       "4  [dont, compare, name, brand, wipes, family, li...   \n",
       "5  [dont, compare, name, brand, wipes, family, li...   \n",
       "6  [dont, compare, name, brand, wipes, family, li...   \n",
       "7                                             [good]   \n",
       "8  [extremely, hard, swallow, pills, huge, sides,...   \n",
       "9  [first, style, leaving, review, solimo, incont...   \n",
       "\n",
       "               Review_Title_tokens  \\\n",
       "0                        [useless]   \n",
       "1                        [useless]   \n",
       "2  [trash, buy, bags, waist, time]   \n",
       "3                      [recommend]   \n",
       "4      [cant, tackle, big, messes]   \n",
       "5      [cant, tackle, big, messes]   \n",
       "6      [cant, tackle, big, messes]   \n",
       "7               [tastes, horrible]   \n",
       "8                [choking, hazard]   \n",
       "9        [bring, back, old, style]   \n",
       "\n",
       "                                     combined_tokens  \n",
       "0  [nothing, didnt, help, lost, even, working, ea...  \n",
       "1  [nothing, didnt, help, lost, even, working, ea...  \n",
       "2  [bought, bags, immediately, open, one, put, tr...  \n",
       "3        [gave, allergic, reaction, face, recommend]  \n",
       "4  [dont, compare, name, brand, wipes, family, li...  \n",
       "5  [dont, compare, name, brand, wipes, family, li...  \n",
       "6  [dont, compare, name, brand, wipes, family, li...  \n",
       "7                           [good, tastes, horrible]  \n",
       "8  [extremely, hard, swallow, pills, huge, sides,...  \n",
       "9  [first, style, leaving, review, solimo, incont...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = stopwords.words('english')\n",
    "\n",
    "# Tokenizing the two columns Review_text, Review_Title\n",
    "for col in ['Review_text', 'Review_Title']:\n",
    "    #Converting the columns to lowercase\n",
    "    DF_prod[col] = DF_prod[col].apply(lambda x :  x.lower())\n",
    "\n",
    "    #Replacing the puncutations\n",
    "    DF_prod[col] = DF_prod[col].str.replace('[^\\w\\s]','')\n",
    "    \n",
    "    # Removing stop words\n",
    "    DF_prod[col] = DF_prod[col].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "    \n",
    "    # Remove numerics from the data\n",
    "    DF_prod[col] = DF_prod[col].str.replace('\\d+', '')\n",
    "    \n",
    "    #Getting the tokens for the two columns\n",
    "    new_col = col + '_tokens'\n",
    "    DF_prod[new_col] = DF_prod.apply(lambda row : word_tokenize(row[col]), axis = 1)\n",
    "\n",
    "# Combined tokens\n",
    "DF_prod['combined_tokens'] = DF_prod['Review_text_tokens'] + DF_prod['Review_Title_tokens']\n",
    "DF_prod.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the data back to train prod and test prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5959, 6) (2553, 6)\n"
     ]
    }
   ],
   "source": [
    "train_prod = DF_prod.loc[DF_prod.topic != 'Test_dataset',].reset_index(drop = True)\n",
    "test_prod = DF_prod.loc[DF_prod.topic == 'Test_dataset',].reset_index(drop = True)\n",
    "\n",
    "print(train_prod.shape, test_prod.shape)\n",
    "\n",
    "# Get the indep and dep features\n",
    "indep = ['combined_tokens']\n",
    "dep = ['topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert the labels to integer\n",
    "LE = LabelEncoder()\n",
    "LE.fit(train_prod.topic)\n",
    "train_prod.topic = LE.transform(train_prod.topic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Splitting to local train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4172, 1) (4172, 1) (1788, 1) (1788, 1) (5959, 1) (2553, 1)\n"
     ]
    }
   ],
   "source": [
    "train_stop = int(train_prod.shape[0]*0.7)\n",
    "\n",
    "np.random.seed(100)\n",
    "# train_local_X, test_local_X, train_local_Y, test_local_Y = train_test_split(train_prod[indep],\n",
    "#                                                                             train_prod[dep],\n",
    "#                                                                             test_size = 0.2)\n",
    "train_local_X = train_prod.loc[0 : train_stop, indep]\n",
    "train_local_Y = train_prod.loc[0 : train_stop, dep]\n",
    "test_local_X = train_prod.loc[train_stop : train_prod.shape[0], indep]\n",
    "test_local_Y = train_prod.loc[train_stop : train_prod.shape[0], dep]\n",
    "\n",
    "train_prod_X = train_prod[indep]\n",
    "train_prod_Y = train_prod[dep]\n",
    "test_prod_X = test_prod[indep]\n",
    "\n",
    "print(train_local_X.shape, train_local_Y.shape, test_local_X.shape, test_local_Y.shape, train_prod_X.shape, test_prod_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download the Bert model for the first time using proxy\n",
    "#os.environ[\"https_proxy\"] = \"http://19.12.1.40:83\"\n",
    "\n",
    "#bert_embedding = BertEmbedding(model='bert_24_1024_16', dataset_name='book_corpus_wiki_en_cased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get the word vectors for the sentence\n",
    "# trian_prod_bert = bert_embedding(train_prod.Review_text)\n",
    "# test_prod_bert = bert_embedding(test_prod.Review_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BTHANISH\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\core\\fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "C:\\Users\\BTHANISH\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\core\\_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([array([-0.04575083,  0.26852962, -0.40489498, ...,  0.11674985,\n",
       "         -0.02588899,  0.16120338], dtype=float32),\n",
       "  array([-0.04575083,  0.26852962, -0.40489498, ...,  0.11674985,\n",
       "         -0.02588899,  0.16120338], dtype=float32),\n",
       "  array([ 0.33884838, -0.06202189, -0.49974066, ..., -0.3031003 ,\n",
       "         -0.02780935, -0.16894451], dtype=float32)],\n",
       " [array([-0.11446831,  0.14398797, -0.210073  , ...,  0.24956672,\n",
       "          0.1559298 , -0.02459837], dtype=float32),\n",
       "  array([-0.11446831,  0.14398797, -0.210073  , ...,  0.24956672,\n",
       "          0.1559298 , -0.02459837], dtype=float32),\n",
       "  array([-0.15249276, -0.47609544, -0.75302124, ...,  0.01009943,\n",
       "         -0.24685921,  0.5926263 ], dtype=float32)])"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the sentence level embedding by taking the mean of each words in the test\n",
    "train_bert_vectors = ([np.mean(trian_prod_bert[i][1], axis = 0) for i in range(len(trian_prod_bert))])\n",
    "train_bert_vectors[5560] = np.array([0.0] *1024) #since it's a null text in that index replacing it with 0\n",
    "\n",
    "test_bert_vectors = ([np.mean(test_prod_bert[i][1], axis = 0) for i in range(len(test_prod_bert))])\n",
    "train_bert_vectors[0:3], test_bert_vectors[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for i in range(np.array(test_prod_bert).shape[0]):\n",
    "#     print(np.mean(test_prod_bert[i][1], axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# n_dim = 1024\n",
    "\n",
    "# #from sklearn.preprocessing import scale\n",
    "# print('Train prod happening')\n",
    "# train_prod_vecs_w2v = np.concatenate([buildWordVector(row[indep], bert_length = 100) for i, row in train_prod_X.iterrows()])\n",
    "# #train_vecs_w2v = scale(train_vecs_w2v)\n",
    "\n",
    "# print('Test prod happening')\n",
    "# test_prod_vecs_w2v = np.concatenate([buildWordVector(row[indep], bert_length = 100) for i, row in test_prod_X.iterrows()])\n",
    "# #test_vecs_w2v = scale(test_vecs_w2v)\n",
    "\n",
    "# print('Train local happening')\n",
    "# train_local_vecs_w2v = np.concatenate([buildWordVector(row[indep], bert_length = 100) for i, row in train_local_X.iterrows()])\n",
    "# #train_vecs_w2v = scale(train_vecs_w2v)\n",
    "\n",
    "# print('Test local happening')\n",
    "# test_local_vecs_w2v = np.concatenate([buildWordVector(row[indep], bert_length = 100) for i, row in test_local_X.iterrows()])\n",
    "# #test_vecs_w2v = scale(test_vecs_w2v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4171, 1024) (4171,) (1788, 1024) (1788,) (5959, 1024) (2553,)\n"
     ]
    }
   ],
   "source": [
    "train_stop = int(train_prod.shape[0]*0.7)\n",
    "\n",
    "np.random.seed(100)\n",
    "# train_local_X, test_local_X, train_local_Y, test_local_Y = train_test_split(train_prod[indep],\n",
    "#                                                                             train_prod[dep],\n",
    "#                                                                             test_size = 0.2)\n",
    "train_local_X = np.array(train_bert_vectors[0 : train_stop])\n",
    "train_local_Y = train_prod.topic[0 : train_stop]\n",
    "\n",
    "test_local_X = np.array(train_bert_vectors[train_stop : train_prod.shape[0]])\n",
    "test_local_Y = train_prod.topic[train_stop : train_prod.shape[0]]\n",
    "\n",
    "train_prod_X = np.array(train_bert_vectors)\n",
    "train_prod_Y = train_prod.topic\n",
    "test_prod_X = np.array(test_bert_vectors)\n",
    "\n",
    "print(train_local_X.shape, train_local_Y.shape, \n",
    "      test_local_X.shape, test_local_Y.shape, train_prod_X.shape, test_prod_X.shape\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtrain_local = xgb.DMatrix(data = train_local_X, label = train_local_Y)\n",
    "dtest_local = xgb.DMatrix(data = test_local_X, label = test_local_Y)\n",
    "#dtrain_prod = xgb.DMatrix(data = train_prod_X, label = train_prod_Y)\n",
    "#dtest_prod = xgb.DMatrix(data = test_prod_X)\n",
    "\n",
    "\n",
    "num_rounds = 1000\n",
    "\n",
    "params = {'objective' : 'multi:softprob',\n",
    "          'num_class' : len(train_local_Y.unique()),\n",
    "          #'eval_metric': 'auc',\n",
    "          'max_depth' : 7,\n",
    "          'eta' : 0.3,\n",
    "          'subsample': 1,\n",
    "          'colsample_bytree': 1,\n",
    "          'silent' : 1\n",
    "          ,'tree_method' : 'gpu_hist'\n",
    "          }\n",
    "\n",
    "eval_set = [(dtrain_local,'train'), (dtest_local,'test')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stopping. Best iteration:\n",
    "#[106]\ttrain-merror:0.288111\ttest-merror:0.488255\n",
    "        \n",
    "np.random.seed(100)\n",
    "num_rounds = 1000\n",
    "np.random.seed(100)\n",
    "xgb_model = xgb.train(params,\n",
    "                      dtrain_local,\n",
    "                      evals = eval_set,\n",
    "                      num_boost_round = num_rounds,\n",
    "                      #feval = custom_mse,\n",
    "                      verbose_eval = True,\n",
    "                      early_stopping_rounds = 30)\n",
    "\n",
    "xgb_local_pred = xgb_model.predict(dtest_local)\n",
    "\n",
    "#Feature importance\n",
    "pd.DataFrame.from_dict(xgb_model.get_score(), orient = 'index').rename(columns = {0:'importance'}).sort_values(['importance'], ascending = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
