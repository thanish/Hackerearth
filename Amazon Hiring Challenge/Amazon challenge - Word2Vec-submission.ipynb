{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandasql as ps\n",
    "from tqdm import tqdm \n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "\n",
    "import gensim \n",
    "import string\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "LabeledSentence = gensim.models.doc2vec.LabeledSentence\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "os.chdir('C:\\\\Users\\\\BTHANISH\\\\Documents\\\\Thanish\\\\Competition\\\\Hacker earth\\\\Amazon Hiring Challenge')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5959, 3) (2553, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review_text</th>\n",
       "      <th>Review_Title</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Did nothing for me, didn't help lost even with...</td>\n",
       "      <td>Useless</td>\n",
       "      <td>Shipment and delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Did nothing for me, didn't help lost even with...</td>\n",
       "      <td>Useless</td>\n",
       "      <td>Not Effective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I have bought these bags and  immediately open...</td>\n",
       "      <td>TRASH!!! Do not buy these bags it’s a waist of...</td>\n",
       "      <td>Customer Service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gave me an allergic reaction on my face :(</td>\n",
       "      <td>Do not recommend</td>\n",
       "      <td>Allergic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>These don't compare to the name brand wipes. F...</td>\n",
       "      <td>Can't tackle big messes</td>\n",
       "      <td>Texture</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Review_text  \\\n",
       "0  Did nothing for me, didn't help lost even with...   \n",
       "1  Did nothing for me, didn't help lost even with...   \n",
       "2  I have bought these bags and  immediately open...   \n",
       "3         Gave me an allergic reaction on my face :(   \n",
       "4  These don't compare to the name brand wipes. F...   \n",
       "\n",
       "                                        Review_Title                  topic  \n",
       "0                                            Useless  Shipment and delivery  \n",
       "1                                            Useless          Not Effective  \n",
       "2  TRASH!!! Do not buy these bags it’s a waist of...       Customer Service  \n",
       "3                                   Do not recommend               Allergic  \n",
       "4                            Can't tackle big messes                Texture  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF_train_prod = pd.read_csv('train.csv')\n",
    "DF_test_prod = pd.read_csv('test.csv')\n",
    "#DF_test_prod = DF_test_prod.loc[~DF_test_prod.duplicated(),]\n",
    "\n",
    "print(DF_train_prod.shape, DF_test_prod.shape)\n",
    "\n",
    "DF_test_prod['topic'] = 'Test_dataset'\n",
    "DF_prod = pd.concat([DF_train_prod, DF_test_prod]).reset_index(drop = True)\n",
    "DF_prod.columns = ['Review_text', 'Review_Title', 'topic']\n",
    "DF_prod.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review_text</th>\n",
       "      <th>Review_Title</th>\n",
       "      <th>topic</th>\n",
       "      <th>Review_text_tokens</th>\n",
       "      <th>Review_Title_tokens</th>\n",
       "      <th>combined_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nothing didnt help lost even working eating he...</td>\n",
       "      <td>useless</td>\n",
       "      <td>Shipment and delivery</td>\n",
       "      <td>[nothing, didnt, help, lost, even, working, ea...</td>\n",
       "      <td>[useless]</td>\n",
       "      <td>[nothing, didnt, help, lost, even, working, ea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nothing didnt help lost even working eating he...</td>\n",
       "      <td>useless</td>\n",
       "      <td>Not Effective</td>\n",
       "      <td>[nothing, didnt, help, lost, even, working, ea...</td>\n",
       "      <td>[useless]</td>\n",
       "      <td>[nothing, didnt, help, lost, even, working, ea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bought bags immediately open one put trash bag...</td>\n",
       "      <td>trash buy bags waist time</td>\n",
       "      <td>Customer Service</td>\n",
       "      <td>[bought, bags, immediately, open, one, put, tr...</td>\n",
       "      <td>[trash, buy, bags, waist, time]</td>\n",
       "      <td>[bought, bags, immediately, open, one, put, tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gave allergic reaction face</td>\n",
       "      <td>recommend</td>\n",
       "      <td>Allergic</td>\n",
       "      <td>[gave, allergic, reaction, face]</td>\n",
       "      <td>[recommend]</td>\n",
       "      <td>[gave, allergic, reaction, face, recommend]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dont compare name brand wipes family  little k...</td>\n",
       "      <td>cant tackle big messes</td>\n",
       "      <td>Texture</td>\n",
       "      <td>[dont, compare, name, brand, wipes, family, li...</td>\n",
       "      <td>[cant, tackle, big, messes]</td>\n",
       "      <td>[dont, compare, name, brand, wipes, family, li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dont compare name brand wipes family  little k...</td>\n",
       "      <td>cant tackle big messes</td>\n",
       "      <td>Quality/Contaminated</td>\n",
       "      <td>[dont, compare, name, brand, wipes, family, li...</td>\n",
       "      <td>[cant, tackle, big, messes]</td>\n",
       "      <td>[dont, compare, name, brand, wipes, family, li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dont compare name brand wipes family  little k...</td>\n",
       "      <td>cant tackle big messes</td>\n",
       "      <td>Color and texture</td>\n",
       "      <td>[dont, compare, name, brand, wipes, family, li...</td>\n",
       "      <td>[cant, tackle, big, messes]</td>\n",
       "      <td>[dont, compare, name, brand, wipes, family, li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>good</td>\n",
       "      <td>tastes horrible</td>\n",
       "      <td>Bad Taste/Flavor</td>\n",
       "      <td>[good]</td>\n",
       "      <td>[tastes, horrible]</td>\n",
       "      <td>[good, tastes, horrible]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>extremely hard swallow pills huge sides sharp ...</td>\n",
       "      <td>choking hazard</td>\n",
       "      <td>Too big to swallow</td>\n",
       "      <td>[extremely, hard, swallow, pills, huge, sides,...</td>\n",
       "      <td>[choking, hazard]</td>\n",
       "      <td>[extremely, hard, swallow, pills, huge, sides,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>first style leaving review solimo incontinent ...</td>\n",
       "      <td>bring back old style</td>\n",
       "      <td>Quality/Contaminated</td>\n",
       "      <td>[first, style, leaving, review, solimo, incont...</td>\n",
       "      <td>[bring, back, old, style]</td>\n",
       "      <td>[first, style, leaving, review, solimo, incont...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Review_text  \\\n",
       "0  nothing didnt help lost even working eating he...   \n",
       "1  nothing didnt help lost even working eating he...   \n",
       "2  bought bags immediately open one put trash bag...   \n",
       "3                        gave allergic reaction face   \n",
       "4  dont compare name brand wipes family  little k...   \n",
       "5  dont compare name brand wipes family  little k...   \n",
       "6  dont compare name brand wipes family  little k...   \n",
       "7                                               good   \n",
       "8  extremely hard swallow pills huge sides sharp ...   \n",
       "9  first style leaving review solimo incontinent ...   \n",
       "\n",
       "                Review_Title                  topic  \\\n",
       "0                    useless  Shipment and delivery   \n",
       "1                    useless          Not Effective   \n",
       "2  trash buy bags waist time       Customer Service   \n",
       "3                  recommend               Allergic   \n",
       "4     cant tackle big messes                Texture   \n",
       "5     cant tackle big messes   Quality/Contaminated   \n",
       "6     cant tackle big messes      Color and texture   \n",
       "7            tastes horrible       Bad Taste/Flavor   \n",
       "8             choking hazard     Too big to swallow   \n",
       "9       bring back old style   Quality/Contaminated   \n",
       "\n",
       "                                  Review_text_tokens  \\\n",
       "0  [nothing, didnt, help, lost, even, working, ea...   \n",
       "1  [nothing, didnt, help, lost, even, working, ea...   \n",
       "2  [bought, bags, immediately, open, one, put, tr...   \n",
       "3                   [gave, allergic, reaction, face]   \n",
       "4  [dont, compare, name, brand, wipes, family, li...   \n",
       "5  [dont, compare, name, brand, wipes, family, li...   \n",
       "6  [dont, compare, name, brand, wipes, family, li...   \n",
       "7                                             [good]   \n",
       "8  [extremely, hard, swallow, pills, huge, sides,...   \n",
       "9  [first, style, leaving, review, solimo, incont...   \n",
       "\n",
       "               Review_Title_tokens  \\\n",
       "0                        [useless]   \n",
       "1                        [useless]   \n",
       "2  [trash, buy, bags, waist, time]   \n",
       "3                      [recommend]   \n",
       "4      [cant, tackle, big, messes]   \n",
       "5      [cant, tackle, big, messes]   \n",
       "6      [cant, tackle, big, messes]   \n",
       "7               [tastes, horrible]   \n",
       "8                [choking, hazard]   \n",
       "9        [bring, back, old, style]   \n",
       "\n",
       "                                     combined_tokens  \n",
       "0  [nothing, didnt, help, lost, even, working, ea...  \n",
       "1  [nothing, didnt, help, lost, even, working, ea...  \n",
       "2  [bought, bags, immediately, open, one, put, tr...  \n",
       "3        [gave, allergic, reaction, face, recommend]  \n",
       "4  [dont, compare, name, brand, wipes, family, li...  \n",
       "5  [dont, compare, name, brand, wipes, family, li...  \n",
       "6  [dont, compare, name, brand, wipes, family, li...  \n",
       "7                           [good, tastes, horrible]  \n",
       "8  [extremely, hard, swallow, pills, huge, sides,...  \n",
       "9  [first, style, leaving, review, solimo, incont...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = stopwords.words('english')\n",
    "\n",
    "# Tokenizing the two columns Review_text, Review_Title\n",
    "for col in ['Review_text', 'Review_Title']:\n",
    "    #Converting the columns to lowercase\n",
    "    DF_prod[col] = DF_prod[col].apply(lambda x :  x.lower())\n",
    "\n",
    "    #Replacing the puncutations\n",
    "    DF_prod[col] = DF_prod[col].str.replace('[^\\w\\s]','')\n",
    "    \n",
    "    # Removing stop words\n",
    "    DF_prod[col] = DF_prod[col].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "    \n",
    "    # Removing stop words\n",
    "    DF_prod[col] = DF_prod[col].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "    \n",
    "    # Remove numerics from the data\n",
    "    DF_prod[col] = DF_prod[col].str.replace('\\d+', '')\n",
    "    \n",
    "    #Getting the tokens for the two columns\n",
    "    new_col = col + '_tokens'\n",
    "    DF_prod[new_col] = DF_prod.apply(lambda row : word_tokenize(row[col]), axis = 1)\n",
    "\n",
    "# Combined tokens\n",
    "DF_prod['combined_tokens'] = DF_prod['Review_text_tokens'] + DF_prod['Review_Title_tokens']\n",
    "DF_prod.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the data back to train prod and test prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5959, 6) (2553, 6)\n"
     ]
    }
   ],
   "source": [
    "train_prod = DF_prod.loc[DF_prod.topic != 'Test_dataset',].reset_index(drop = True)\n",
    "test_prod = DF_prod.loc[DF_prod.topic == 'Test_dataset',].reset_index(drop = True)\n",
    "\n",
    "print(train_prod.shape, test_prod.shape)\n",
    "\n",
    "# Get the indep and dep features\n",
    "indep = ['combined_tokens']\n",
    "dep = ['topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert the labels to integer\n",
    "LE = LabelEncoder()\n",
    "LE.fit(train_prod.topic)\n",
    "train_prod.topic = LE.transform(train_prod.topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4172, 1) (4172, 1) (1788, 1) (1788, 1) (5959, 1) (2553, 1)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(100)\n",
    "# train_local_X, test_local_X, train_local_Y, test_local_Y = train_test_split(train_prod[indep],\n",
    "#                                                                             train_prod[dep],\n",
    "#                                                                             test_size = 0.2)\n",
    "\n",
    "\n",
    "# print(train_prod.shape)\n",
    "# print(train_prod.loc[~train_prod.Review_text.duplicated(),].shape)\n",
    "# train_prod = train_prod.loc[~train_prod.Review_text.duplicated(),]\n",
    "\n",
    "\n",
    "train_stop = int(train_prod.shape[0]*0.7)\n",
    "\n",
    "train_local_X = train_prod.loc[0 : train_stop, indep]\n",
    "train_local_Y = train_prod.loc[0 : train_stop, dep]\n",
    "test_local_X = train_prod.loc[train_stop : train_prod.shape[0], indep]\n",
    "test_local_Y = train_prod.loc[train_stop : train_prod.shape[0], dep]\n",
    "\n",
    "train_prod_X = train_prod[indep]\n",
    "train_prod_Y = train_prod[dep]\n",
    "test_prod_X = test_prod[indep]\n",
    "\n",
    "print(train_local_X.shape, train_local_Y.shape, test_local_X.shape, test_local_Y.shape, train_prod_X.shape, test_prod_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Giving labels before building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     combined_tokens\n",
      "0  [nothing, didnt, help, lost, even, working, ea...\n",
      "1  [nothing, didnt, help, lost, even, working, ea...\n",
      "[LabeledSentence(words=['nothing', 'didnt', 'help', 'lost', 'even', 'working', 'eating', 'healthy', 'didnt', 'curb', 'appetite', 'anything', 'useless'], tags=['TRAIN_local_0']), LabeledSentence(words=['nothing', 'didnt', 'help', 'lost', 'even', 'working', 'eating', 'healthy', 'didnt', 'curb', 'appetite', 'anything', 'useless'], tags=['TRAIN_local_1'])]\n",
      "\n",
      "                                     combined_tokens\n",
      "0  [use, chia, seed, protein, shakes, tasted, lik...\n",
      "1  [use, chia, seed, protein, shakes, tasted, lik...\n",
      "[LabeledSentence(words=['use', 'chia', 'seed', 'protein', 'shakes', 'tasted', 'like', 'moldy', 'throw', 'bad', 'tast'], tags=['TEST_prod_0']), LabeledSentence(words=['use', 'chia', 'seed', 'protein', 'shakes', 'tasted', 'like', 'moldy', 'throw', 'bad', 'tast'], tags=['TEST_prod_1'])]\n"
     ]
    }
   ],
   "source": [
    "#Giving Label to the tokens before passing them to the model\n",
    "def labelizetokens(tokens, label_type):\n",
    "    labelized = []\n",
    "    for i,v in (enumerate(tokens)):\n",
    "        label = '%s_%s'%(label_type,i)\n",
    "        labelized.append(LabeledSentence(v, [label]))\n",
    "    return labelized\n",
    "\n",
    "x_train_local = labelizetokens(train_local_X['combined_tokens'], 'TRAIN_local')\n",
    "x_test_local = labelizetokens(test_local_X['combined_tokens'], 'TEST_local')\n",
    "\n",
    "x_train_prod = labelizetokens(train_prod_X['combined_tokens'], 'TRAIN_prod')\n",
    "x_test_prod = labelizetokens(test_prod_X['combined_tokens'], 'TEST_prod')\n",
    "\n",
    "print(train_local_X.head(2))\n",
    "print(x_train_local[0:2])\n",
    "print(\"\")\n",
    "print(test_prod_X.head(2))\n",
    "print(x_test_prod[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6887319"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # ON train local\n",
    "# n_dim = 200\n",
    "# model = Word2Vec(size = n_dim, min_count= 10)\n",
    "# model.build_vocab([x.words for x in (x_train_local)])\n",
    "# model.train([x.words for x in (x_train_local)], total_examples= model.corpus_count, epochs=100)\n",
    "\n",
    "# ON train prod\n",
    "n_dim = 200\n",
    "model = Word2Vec(size = n_dim, min_count= 10)\n",
    "model.build_vocab([x.words for x in (x_train_prod)])\n",
    "model.train([x.words for x in (x_train_prod)], total_examples= model.corpus_count, epochs=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a tf-idf vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tf-idf matrix ...\n",
      "vocab size : 1967\n"
     ]
    }
   ],
   "source": [
    "# #on train local\n",
    "# print('building tf-idf matrix ...')\n",
    "# vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=10)\n",
    "# matrix = vectorizer.fit_transform([x.words for x in x_train_local])\n",
    "# tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "# print('vocab size :', len(tfidf))\n",
    "\n",
    "#on train prod\n",
    "print('building tf-idf matrix ...')\n",
    "vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=10)\n",
    "matrix = vectorizer.fit_transform([x.words for x in x_train_prod])\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "print('vocab size :', len(tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to create average vector from the tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildWordVector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += model[word].reshape((1, size)) * tfidf[word]\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not\n",
    "                         # in the corpus. useful for testing.\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import scale\n",
    "train_prod_vecs_w2v = np.concatenate([buildWordVector(z, n_dim) for z in (map(lambda x: x.words, x_train_prod))])\n",
    "#train_vecs_w2v = scale(train_vecs_w2v)\n",
    "\n",
    "train_local_vecs_w2v = np.concatenate([buildWordVector(z, n_dim) for z in (map(lambda x: x.words, x_train_local))])\n",
    "#train_vecs_w2v = scale(train_vecs_w2v)\n",
    "\n",
    "test_local_vecs_w2v = np.concatenate([buildWordVector(z, n_dim) for z in (map(lambda x: x.words, x_test_local))])\n",
    "#test_vecs_w2v = scale(test_vecs_w2v)\n",
    "\n",
    "test_prod_vecs_w2v = np.concatenate([buildWordVector(z, n_dim) for z in (map(lambda x: x.words, x_test_prod))])\n",
    "#test_vecs_w2v = scale(test_vecs_w2v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LabeledSentence(words=['nothing', 'didnt', 'help', 'lost', 'even', 'working', 'eating', 'healthy', 'didnt', 'curb', 'appetite', 'anything', 'useless'], tags=['TRAIN_local_0']),\n",
       " LabeledSentence(words=['ive', 'received', 'minutes', 'ago', 'opened', 'box', 'find', 'oil', 'completely', 'melted', 'leakedspilled', 'box', 'came', 'upsetting', 'x', 'brand', 'packed', 'products', 'purchased', 'oil', 'got', 'everythingbr', 'br', 'tried', 'return', 'wasnt', 'option', 'gave', 'link', 'products', 'customer', 'support', 'clicked', 'link', 'came', 'message', 'url', 'cant', 'shown', 'leaked', 'delivery', 'box', 'customer', 'supportno', 'refund', 'option'], tags=['TEST_local_0']),\n",
       " LabeledSentence(words=['nothing', 'didnt', 'help', 'lost', 'even', 'working', 'eating', 'healthy', 'didnt', 'curb', 'appetite', 'anything', 'useless'], tags=['TRAIN_prod_0']),\n",
       " LabeledSentence(words=['use', 'chia', 'seed', 'protein', 'shakes', 'tasted', 'like', 'moldy', 'throw', 'bad', 'tast'], tags=['TEST_prod_0']))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_local[0], x_test_local[0], x_train_prod[0], x_test_prod[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4172, 200) (1788, 200) (5959, 200) (2553, 200)\n"
     ]
    }
   ],
   "source": [
    "print(train_local_vecs_w2v.shape, test_local_vecs_w2v.shape, train_prod_vecs_w2v.shape, test_prod_vecs_w2v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tsne_plot(model):\n",
    "    \"Creates and TSNE model and plots it\"\n",
    "    labels = []\n",
    "    tokens = []\n",
    "\n",
    "    for word in model.wv.vocab:\n",
    "        tokens.append(model[word])\n",
    "        labels.append(word)\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(16, 16)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_plot(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtrain_local = xgb.DMatrix(data = train_local_vecs_w2v, label = train_local_Y[dep])\n",
    "dtest_local = xgb.DMatrix(data = test_local_vecs_w2v, label = test_local_Y[dep])\n",
    "dtrain_prod = xgb.DMatrix(data = train_prod_vecs_w2v, label = train_prod_Y[dep])\n",
    "dtest_prod = xgb.DMatrix(data = test_prod_vecs_w2v)\n",
    "\n",
    "\n",
    "num_rounds = 1000\n",
    "\n",
    "params = {'objective' : 'multi:softprob',\n",
    "          'num_class' : len(train_local_Y.topic.unique()),\n",
    "          #'eval_metric': 'auc',\n",
    "          'max_depth' : 7,\n",
    "          'eta' : 0.3,\n",
    "          'subsample': 1,\n",
    "          'colsample_bytree': 1,\n",
    "          'silent' : 1\n",
    "          ,'tree_method' : 'gpu_hist'\n",
    "          }\n",
    "\n",
    "eval_set = [(dtrain_local,'train'), (dtest_local,'test')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Stopping. Best iteration:\n",
    "#[106]\ttrain-merror:0.288111\ttest-merror:0.488255\n",
    "        \n",
    "np.random.seed(100)\n",
    "num_rounds = 1000\n",
    "np.random.seed(100)\n",
    "xgb_model = xgb.train(params,\n",
    "                      dtrain_local,\n",
    "                      evals = eval_set,\n",
    "                      num_boost_round = num_rounds,\n",
    "                      #feval = custom_mse,\n",
    "                      verbose_eval = True,\n",
    "                      early_stopping_rounds = 30)\n",
    "\n",
    "xgb_local_pred = xgb_model.predict(dtest_local)\n",
    "\n",
    "#Feature importance\n",
    "pd.DataFrame.from_dict(xgb_model.get_score(), orient = 'index').rename(columns = {0:'importance'}).sort_values(['importance'], ascending = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "xgb_model_prod = xgb.train(params,\n",
    "                           dtrain_prod,\n",
    "                           evals = eval_set,\n",
    "                           num_boost_round = xgb_model.best_iteration + 10\n",
    "                           #feval = custom_mse,\n",
    "                           #verbose_eval = True,\n",
    "                           #early_stopping_rounds = 20\n",
    "                          )\n",
    "\n",
    "\n",
    "xgb_prod_pred = xgb_model_prod.predict(dtest_prod)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGB prod submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BTHANISH\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2553, 3) (1772, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Review Title</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I use chia seed in my protein shakes. These ta...</td>\n",
       "      <td>Bad tast</td>\n",
       "      <td>Bad Taste/Flavor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I use chia seed in my protein shakes. These ta...</td>\n",
       "      <td>Bad tast</td>\n",
       "      <td>Quality/Contaminated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Don’t waste your money.</td>\n",
       "      <td>No change. No results.</td>\n",
       "      <td>Not Effective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I use the book 'Fortify Your Life' by Tieraona...</td>\n",
       "      <td>Good Vegan Choice, Poor Non Vegan Choice</td>\n",
       "      <td>Ingredients</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I use the book 'Fortify Your Life' by Tieraona...</td>\n",
       "      <td>Good Vegan Choice, Poor Non Vegan Choice</td>\n",
       "      <td>Quality/Contaminated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Review Text  \\\n",
       "0  I use chia seed in my protein shakes. These ta...   \n",
       "1  I use chia seed in my protein shakes. These ta...   \n",
       "2                            Don’t waste your money.   \n",
       "3  I use the book 'Fortify Your Life' by Tieraona...   \n",
       "4  I use the book 'Fortify Your Life' by Tieraona...   \n",
       "\n",
       "                               Review Title                 topic  \n",
       "0                                  Bad tast      Bad Taste/Flavor  \n",
       "1                                  Bad tast  Quality/Contaminated  \n",
       "2                    No change. No results.         Not Effective  \n",
       "3  Good Vegan Choice, Poor Non Vegan Choice           Ingredients  \n",
       "4  Good Vegan Choice, Poor Non Vegan Choice  Quality/Contaminated  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#XGB_prod_pred = xgb_model_prod.predict_proba(test_prod_vecs_w2v)\n",
    "DF_test_prod = pd.read_csv('test.csv')\n",
    "\n",
    "top_n_predictions = np.fliplr(np.argsort(xgb_prod_pred, axis = 1))\n",
    "top_class = LE.inverse_transform(top_n_predictions)\n",
    "DF_test_prod['topic'] = top_class.tolist()\n",
    "DF_test_prod = DF_test_prod.loc[~DF_test_prod['Review Text'].duplicated(),]\n",
    "\n",
    "DF_test_prod.head()\n",
    "\n",
    "submission = pd.read_csv('Test.csv')\n",
    "submission['seq'] = submission.groupby(['Review Text']).cumcount()\n",
    "\n",
    "print(submission.shape, DF_test_prod.shape)\n",
    "#submission.head()\n",
    "\n",
    "submission = submission.merge(DF_test_prod[['Review Text', 'topic']], how = 'left',\n",
    "                              left_on = ['Review Text'], right_on = ['Review Text'] )\n",
    "submission.head(10)\n",
    "\n",
    "final_list = []\n",
    "for index, row in submission.iterrows():\n",
    "#     print(index)\n",
    "#     print(submission['seq'][index])\n",
    "#     print(\"\")\n",
    "#     print(submission['topic'][index][submission['seq'][index]])\n",
    "    temp = submission['topic'][index][submission['seq'][index]]\n",
    "#   temp = submission['topic'][index][0]\n",
    "    final_list.append(temp)\n",
    "submission['topic'] = final_list\n",
    "#submission['topic'] = LE.inverse_transform(submission['topic'])\n",
    "submission.drop(['seq'], axis =1, inplace = True)\n",
    "\n",
    "submission.to_csv('XGB_sub_10.csv', index = False)\n",
    "submission.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
